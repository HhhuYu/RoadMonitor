{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-14T09:06:22.381472Z",
     "start_time": "2020-05-14T09:06:16.870598Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import pywt\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import defaultdict, Counter\n",
    "import keras\n",
    "from keras.layers import Dense, Flatten\n",
    "from keras.layers import Conv2D, MaxPooling2D\n",
    "from keras.models import Sequential\n",
    "from keras.callbacks import History \n",
    "from sklearn.model_selection import train_test_split\n",
    "history = History()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 加载数据文件"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-14T09:06:22.388429Z",
     "start_time": "2020-05-14T09:06:22.382445Z"
    }
   },
   "outputs": [],
   "source": [
    "typeDescription = {\n",
    "    1: 'normal',\n",
    "    2: 'crack',\n",
    "    3: 'manhole',\n",
    "    4: 'hole',\n",
    "    5: 'patch',\n",
    "    6: 'bump',\n",
    "    7: 'junction',\n",
    "    8: 'rough',\n",
    "    9: 'bridge'\n",
    "}\n",
    "\n",
    "\n",
    "def readFile(filename):\n",
    "    return np.loadtxt(filename)\n",
    "\n",
    "def randomize(dataset, labels):\n",
    "    permutation = np.random.permutation(labels.shape[0])\n",
    "    shuffled_dataset = dataset[permutation, :, :]\n",
    "    shuffled_labels = labels[permutation]\n",
    "    return shuffled_dataset, shuffled_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-14T09:09:08.733144Z",
     "start_time": "2020-05-14T09:09:07.277861Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "数据集： (5717, 128, 3)\n",
      "{'normal': 4994, 'crack': 40, 'manhole': 100, 'hole': 102, 'patch': 142, 'bridge': 5, 'bump': 15, 'junction': 31, 'rough': 288}\n"
     ]
    }
   ],
   "source": [
    "folderPath ='../data/7-12/camera/April/datasets/'\n",
    "dataFile = ['dataX.txt', 'dataY.txt', 'dataZ.txt']\n",
    "labelFile = ['labelSeverity.txt', 'labelType.txt','isAnomaly.txt','anomalyPercent.txt']\n",
    "\n",
    "signals = []\n",
    "for file in dataFile:\n",
    "    dataPath = folderPath+file\n",
    "    signals.append(np.loadtxt(dataPath))\n",
    "signals = np.transpose(np.array(signals), (1, 2, 0))\n",
    "print('数据集：',signals.shape)\n",
    "\n",
    "labelTypePath = folderPath+labelFile[1]\n",
    "isAnomalyPath = folderPath+labelFile[2]\n",
    "labelType = np.loadtxt(labelTypePath)\n",
    "isAnomaly = np.loadtxt(isAnomalyPath)\n",
    "\n",
    "dic = {}\n",
    "temp = Counter(labelType)\n",
    "for key in temp.keys():\n",
    "    dic[typeDescription[key]] = temp[key]\n",
    "print(dic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-14T09:09:31.414849Z",
     "start_time": "2020-05-14T09:09:31.400305Z"
    }
   },
   "outputs": [],
   "source": [
    "shuffled_signals, shuffled_labelType  = randomize(signals, labelType)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Applying CWT and extracting features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-14T09:12:27.580812Z",
     "start_time": "2020-05-14T09:09:34.513619Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1000\n",
      "2000\n",
      "3000\n",
      "4000\n",
      "5000\n"
     ]
    }
   ],
   "source": [
    "scales = range(1,128)\n",
    "waveletname = 'morl'\n",
    "num =shuffled_signals.shape[0]\n",
    "data_cwt = np.ndarray(shape=(num, 127, 127, 3),dtype='float32')\n",
    "\n",
    "for ii in range(0,num):\n",
    "    if ii % 1000 == 0:\n",
    "        print(ii)\n",
    "    for jj in range(0,3):\n",
    "        signal = shuffled_signals[ii, :, jj]\n",
    "        coeff, freq = pywt.cwt(signal, scales, waveletname, 1)\n",
    "        #print(coeff.shape)\n",
    "        coeff_ = coeff[:,:127] #coeff.shape = (127,128), coeff_.shape = (127,127)\n",
    "        #print(coeff_.shape)\n",
    "        data_cwt[ii, :, :, jj] = coeff_\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training a CNN "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-14T09:14:28.679752Z",
     "start_time": "2020-05-14T09:14:28.151714Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4116, 127, 127, 3)\n",
      "(1601, 127, 127, 3)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"\\nimg_x = 127\\nimg_y = 127\\nimg_z = 3\\nnum_classes = 10 #类别\\n\\nbatch_size = 16\\nepochs = 10\\n\\n# reshape the data into a 4D tensor - (sample_number, x_img_size, y_img_size, num_channels)\\n# because the MNIST is greyscale, we only have a single channel - RGB colour images would have 3\\ninput_shape = (img_x, img_y, img_z)\\n\\n# convert the data to the right type\\nx_train = x_train.reshape(x_train.shape[0], img_x, img_y, img_z)\\nx_test = x_test.reshape(x_test.shape[0], img_x, img_y, img_z)\\nx_train = x_train.astype('float32')\\nx_test = x_test.astype('float32')\\n\\nprint('x_train shape:', x_train.shape)\\nprint(x_train.shape[0], 'train samples')\\nprint(x_test.shape[0], 'test samples')\\n\\n# convert class vectors to binary class matrices - this is for use in the\\n# categorical_crossentropy loss below\\ny_train = keras.utils.to_categorical(y_train, num_classes)\\ny_test = keras.utils.to_categorical(y_test, num_classes)\\n\""
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train,x_test,y_train,y_test = train_test_split(data_cwt,shuffled_labelType,test_size=0.28,random_state=0)\n",
    "print(x_train.shape)\n",
    "print(x_test.shape)\n",
    "\n",
    "img_x = 127\n",
    "img_y = 127\n",
    "img_z = 3\n",
    "num_classes = 10 #类别\n",
    "\n",
    "batch_size = 16\n",
    "epochs = 10\n",
    "\n",
    "# reshape the data into a 4D tensor - (sample_number, x_img_size, y_img_size, num_channels)\n",
    "# because the MNIST is greyscale, we only have a single channel - RGB colour images would have 3\n",
    "input_shape = (img_x, img_y, img_z)\n",
    "\n",
    "# convert the data to the right type\n",
    "x_train = x_train.reshape(x_train.shape[0], img_x, img_y, img_z)\n",
    "x_test = x_test.reshape(x_test.shape[0], img_x, img_y, img_z)\n",
    "x_train = x_train.astype('float32')\n",
    "x_test = x_test.astype('float32')\n",
    "\n",
    "print('x_train shape:', x_train.shape)\n",
    "print(x_train.shape[0], 'train samples')\n",
    "print(x_test.shape[0], 'test samples')\n",
    "\n",
    "# convert class vectors to binary class matrices - this is for use in the\n",
    "# categorical_crossentropy loss below\n",
    "y_train = keras.utils.to_categorical(y_train, num_classes)\n",
    "y_test = keras.utils.to_categorical(y_test, num_classes)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2020-05-13T13:52:14.404Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From d:\\software\\python\\python-3.7.6\\lib\\site-packages\\tensorflow_core\\python\\ops\\resource_variable_ops.py:1630: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "If using Keras pass *_constraint arguments to layers.\n",
      "WARNING:tensorflow:From d:\\software\\python\\python-3.7.6\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:4070: The name tf.nn.max_pool is deprecated. Please use tf.nn.max_pool2d instead.\n",
      "\n",
      "WARNING:tensorflow:From d:\\software\\python\\python-3.7.6\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:422: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.\n",
      "\n",
      "Train on 4116 samples, validate on 1601 samples\n",
      "Epoch 1/10\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "model.add(Conv2D(32, kernel_size=(5, 5), strides=(1, 1), activation='relu', input_shape=input_shape)) \n",
    "model.add(MaxPooling2D(pool_size=(2, 2), strides=(2, 2)))\n",
    "model.add(Conv2D(64, (5, 5), activation='relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(1000, activation='relu'))\n",
    "model.add(Dense(num_classes, activation='softmax'))\n",
    "\n",
    "model.compile(loss=keras.losses.categorical_crossentropy, \n",
    "              optimizer=keras.optimizers.Adam(), \n",
    "              metrics=['accuracy'])\n",
    "\n",
    "model.fit(x_train, y_train, batch_size=batch_size, \n",
    "          epochs=epochs, verbose=1, \n",
    "          validation_data=(x_test, y_test), \n",
    "          callbacks=[history])\n",
    "\n",
    "train_score = model.evaluate(x_train, y_train, verbose=0)\n",
    "print('Train loss: {}, Train accuracy: {}'.format(train_score[0], train_score[1]))\n",
    "test_score = model.evaluate(x_test, y_test, verbose=0)\n",
    "print('Test loss: {}, Test accuracy: {}'.format(test_score[0], test_score[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FocalLoss(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(FocalLoss, self).__init__()\n",
    "    def forward(self, predictions, target, cuda_device, alpha=0.25, gamma=2):\n",
    "        zeros = torch.zeros(predictions.shape, dtype=predictions.dtype)\n",
    "        if cuda_device != -1:\n",
    "            zeros = zeros.cuda()\n",
    "        pos_corr = torch.where(target > zeros, target - predictions, zeros)\n",
    "        neg_corr = torch.where(target > zeros, zeros, predictions)\n",
    "        fl_loss = - alpha * (pos_corr ** gamma) * torch.log(torch.clamp(predictions, 1e-8, 1.0)) \\\n",
    "                    - (1 - alpha) * (neg_corr ** gamma) * torch.log(torch.clamp(1.0 - predictions, 1e-8, 1.0))\n",
    "        return torch.sum(fl_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-13T13:13:35.109597Z",
     "start_time": "2020-05-13T13:10:32.728Z"
    }
   },
   "outputs": [],
   "source": [
    "fig, axarr = plt.subplots(figsize=(12,6), ncols=2)\n",
    "axarr[0].plot(range(1, 11), history.history['acc'], label='train score')\n",
    "axarr[0].plot(range(1, 11), history.history['val_acc'], label='test score')\n",
    "axarr[0].set_xlabel('Number of Epochs', fontsize=18)\n",
    "axarr[0].set_ylabel('Accuracy', fontsize=18)\n",
    "axarr[0].set_ylim([0,1])\n",
    "axarr[1].plot(range(1, 11), history.history['acc'], label='train score')\n",
    "axarr[1].plot(range(1, 11), history.history['val_acc'], label='test score')\n",
    "axarr[1].set_xlabel('Number of Epochs', fontsize=18)\n",
    "axarr[1].set_ylabel('Accuracy', fontsize=18)\n",
    "axarr[1].set_ylim([0.9,1])\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
