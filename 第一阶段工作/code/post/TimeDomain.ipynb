{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-07T14:38:06.535598Z",
     "start_time": "2020-07-07T14:38:06.529636Z"
    }
   },
   "outputs": [],
   "source": [
    "from siml.sk_utils import *\n",
    "from siml.signal_analysis_utils import *\n",
    "import numpy as np\n",
    "import pywt\n",
    "import scipy.stats\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import warnings\n",
    "from collections import Counter\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import Normalizer\n",
    "from sklearn import svm\n",
    "\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.model_selection import train_test_split    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 加载数据文件"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-07T14:38:06.565090Z",
     "start_time": "2020-07-07T14:38:06.561109Z"
    }
   },
   "outputs": [],
   "source": [
    "typeDescription = {\n",
    "    0: 'normal',\n",
    "    1: 'pothole',\n",
    "    2: 'transverse',\n",
    "}\n",
    "\n",
    "\n",
    "def readFile(filename):\n",
    "    return np.loadtxt(filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-07T14:38:07.221336Z",
     "start_time": "2020-07-07T14:38:06.567083Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "数据集： (4437, 64, 3)\n",
      "{'normal': 3848, 'pothole': 563, 'transverse': 26}\n"
     ]
    }
   ],
   "source": [
    "folderPath = '../../data/Final_Version/all/datasets/'\n",
    "dataFile = ['dataX.txt', 'dataY.txt', 'dataZ.txt']\n",
    "labelFile = ['dataLabel.txt']\n",
    "\n",
    "signals = []\n",
    "for file in dataFile:\n",
    "    dataPath = folderPath+file\n",
    "    signals.append(np.loadtxt(dataPath))\n",
    "signals = np.transpose(np.array(signals), (1, 2, 0))\n",
    "print('数据集：', signals.shape)\n",
    "\n",
    "labelFilePath = folderPath+labelFile[0]\n",
    "dataLabel = np.loadtxt(labelFilePath)\n",
    "anomalyType = list(dataLabel[:, 0])\n",
    "\n",
    "dic = {}\n",
    "temp = Counter(anomalyType)\n",
    "for key in temp.keys():\n",
    "    dic[typeDescription[key]] = temp[key]\n",
    "print(dic)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 时域特征提取"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-07T14:38:07.235296Z",
     "start_time": "2020-07-07T14:38:07.223330Z"
    }
   },
   "outputs": [],
   "source": [
    "def calculate_entropy(list_values):\n",
    "    counter_values = Counter(list_values).most_common()\n",
    "    # print(counter_values)\n",
    "    probabilities = [elem[1]/len(list_values) for elem in counter_values]\n",
    "    # print(probabilities)\n",
    "    entropy = scipy.stats.entropy(probabilities)\n",
    "    return entropy\n",
    "\n",
    "\n",
    "def calculate_statistics(list_values):\n",
    "    n5 = np.nanpercentile(list_values, 5)\n",
    "    n25 = np.nanpercentile(list_values, 25)\n",
    "    n75 = np.nanpercentile(list_values, 75)\n",
    "    n95 = np.nanpercentile(list_values, 95)\n",
    "    median = np.nanpercentile(list_values, 50)\n",
    "    mean = np.nanmean(list_values)\n",
    "    std = np.nanstd(list_values)\n",
    "    var = np.nanvar(list_values)\n",
    "    rms = np.nanmean(np.sqrt(list_values**2))\n",
    "    return [n5, n25, n75, n95, median, mean, std, var, rms]\n",
    "\n",
    "\n",
    "def calculate_crossings(list_values):\n",
    "    zero_crossing_indices = np.nonzero(np.diff(np.array(list_values) > 0))[0]\n",
    "    no_zero_crossings = len(zero_crossing_indices)\n",
    "    mean_crossing_indices = np.nonzero(\n",
    "        np.diff(np.array(list_values) > np.nanmean(list_values)))[0]\n",
    "    no_mean_crossings = len(mean_crossing_indices)\n",
    "    return [no_zero_crossings, no_mean_crossings]\n",
    "\n",
    "\n",
    "def get_features(list_values):\n",
    "    entropy = calculate_entropy(list_values)\n",
    "    crossings = calculate_crossings(list_values)\n",
    "    statistics = calculate_statistics(list_values)\n",
    "    return [entropy] + crossings + statistics\n",
    "\n",
    "\n",
    "def extract_features(dataset):\n",
    "    uci_har_features = []\n",
    "    for signal_no in range(0, len(dataset)):\n",
    "        features = []\n",
    "        for signal_comp in range(0, dataset.shape[2]):\n",
    "            signal = dataset[signal_no, :, signal_comp]\n",
    "            features += get_features(signal)\n",
    "        uci_har_features.append(features)\n",
    "    X = np.array(uci_har_features)\n",
    "    return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-07T14:38:18.579219Z",
     "start_time": "2020-07-07T14:38:07.237289Z"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "features = extract_features(signals)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-07T14:38:18.586172Z",
     "start_time": "2020-07-07T14:38:18.580157Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4437, 36)\n"
     ]
    }
   ],
   "source": [
    "print(features.shape)\n",
    "labels = np.array(anomalyType)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-07T14:38:18.603098Z",
     "start_time": "2020-07-07T14:38:18.588137Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.09125097, 0.35475489, 0.39909925, ..., 0.0609016 , 0.16728191,\n",
       "        0.03384791],\n",
       "       [0.06537597, 0.33781342, 0.33781342, ..., 0.01601879, 0.01595151,\n",
       "        0.01034629],\n",
       "       [0.06601799, 0.35290347, 0.35290347, ..., 0.01404617, 0.01229937,\n",
       "        0.0115611 ],\n",
       "       ...,\n",
       "       [0.07345631, 0.19633274, 0.19633274, ..., 0.02439802, 0.03335103,\n",
       "        0.02030705],\n",
       "       [0.06544915, 0.27034842, 0.27034842, ..., 0.01589514, 0.01588743,\n",
       "        0.01265916],\n",
       "       [0.08932298, 0.28364118, 0.28364118, ..., 0.01656266, 0.01257286,\n",
       "        0.01245703]])"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "StandardScaler().fit_transform(features)\n",
    "Normalizer().fit_transform(features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 信号分类"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-07T14:38:18.611075Z",
     "start_time": "2020-07-07T14:38:18.606093Z"
    }
   },
   "outputs": [],
   "source": [
    "def randomize(features, labels):\n",
    "    permutation = np.random.permutation(labels.shape[0])\n",
    "    shuffled_features = features[permutation, :]\n",
    "    shuffled_labels = labels[permutation]\n",
    "    return shuffled_features, shuffled_labels\n",
    "\n",
    "iteration = 100\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-07T14:38:32.561864Z",
     "start_time": "2020-07-07T14:38:18.613069Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results of Logistic Regression\n",
      "Accuracy on training set is : 0.9522608695652167\n",
      "Accuracy on test set is : 0.9484534534534537\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0      0.958     0.986     0.972    115387\n",
      "         1.0      0.871     0.740     0.800     16999\n",
      "         2.0      0.500     0.012     0.024       814\n",
      "\n",
      "    accuracy                          0.948    133200\n",
      "   macro avg      0.776     0.579     0.599    133200\n",
      "weighted avg      0.944     0.948     0.944    133200\n",
      "\n"
     ]
    }
   ],
   "source": [
    "yTest, yPredict = [], []\n",
    "trainingScore, testingScore = 0, 0\n",
    "for i in range(iteration):\n",
    "    X, Y = randomize(features, labels)\n",
    "    X_train, X_test, Y_train, Y_test = train_test_split(\n",
    "        X, Y, test_size=0.3, random_state=0)\n",
    "    clf = LogisticRegression(random_state=0)\n",
    "    clf.fit(X_train, Y_train)\n",
    "    Y_test_pred = clf.predict(X_test)\n",
    "    yTest.extend(Y_test)\n",
    "    yPredict.extend(Y_test_pred)\n",
    "    trainingScore += clf.score(X_train, Y_train)\n",
    "    testingScore += clf.score(X_test, Y_test)\n",
    "print(\"Results of Logistic Regression\")\n",
    "print(\"Accuracy on training set is : {}\".format(trainingScore/iteration))\n",
    "print(\"Accuracy on test set is : {}\".format(testingScore/iteration))\n",
    "print(classification_report(yTest, yPredict, digits=3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Support Vector Machine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-07T14:39:01.787000Z",
     "start_time": "2020-07-07T14:38:32.562860Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results of Support Vector Machine\n",
      "Accuracy on training set is : 0.92997423510467\n",
      "Accuracy on test set is : 0.9290765765765768\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0      0.926     0.998     0.961    115495\n",
      "         1.0      0.975     0.499     0.660     16908\n",
      "         2.0      0.000     0.000     0.000       797\n",
      "\n",
      "    accuracy                          0.929    133200\n",
      "   macro avg      0.634     0.499     0.540    133200\n",
      "weighted avg      0.927     0.929     0.917    133200\n",
      "\n"
     ]
    }
   ],
   "source": [
    "yTest, yPredict = [], []\n",
    "trainingScore, testingScore = 0, 0\n",
    "for i in range(iteration):\n",
    "    X, Y = randomize(features, labels)\n",
    "    X_train, X_test, Y_train, Y_test = train_test_split(\n",
    "        X, Y, test_size=0.3, random_state=0)\n",
    "    clf = svm.SVC()\n",
    "    clf.fit(X_train, Y_train)\n",
    "    Y_test_pred = clf.predict(X_test)\n",
    "    yTest.extend(Y_test)\n",
    "    yPredict.extend(Y_test_pred)\n",
    "    trainingScore += clf.score(X_train, Y_train)\n",
    "    testingScore += clf.score(X_test, Y_test)\n",
    "print(\"Results of Support Vector Machine\")\n",
    "print(\"Accuracy on training set is : {}\".format(trainingScore/iteration))\n",
    "print(\"Accuracy on test set is : {}\".format(testingScore/iteration))\n",
    "print(classification_report(yTest, yPredict, digits=3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-07T14:40:59.318059Z",
     "start_time": "2020-07-07T14:39:01.787989Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results of RandomForest\n",
      "Accuracy on training set is : 0.999987117552335\n",
      "Accuracy on test set is : 0.9491516516516519\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0      0.959     0.984     0.972    115593\n",
      "         1.0      0.866     0.747     0.802     16854\n",
      "         2.0      1.000     0.058     0.110       753\n",
      "\n",
      "    accuracy                          0.949    133200\n",
      "   macro avg      0.942     0.597     0.628    133200\n",
      "weighted avg      0.948     0.949     0.945    133200\n",
      "\n"
     ]
    }
   ],
   "source": [
    "yTest, yPredict = [], []\n",
    "trainingScore, testingScore = 0, 0\n",
    "for i in range(iteration):\n",
    "    X, Y = randomize(features, labels)\n",
    "    X_train, X_test, Y_train, Y_test = train_test_split(\n",
    "        X, Y, test_size=0.3, random_state=0)\n",
    "    clf = RandomForestClassifier(n_estimators=100)\n",
    "    clf.fit(X_train, Y_train)\n",
    "    Y_test_pred = clf.predict(X_test)\n",
    "    yTest.extend(Y_test)\n",
    "    yPredict.extend(Y_test_pred)\n",
    "    trainingScore += clf.score(X_train, Y_train)\n",
    "    testingScore += clf.score(X_test, Y_test)\n",
    "print(\"Results of RandomForest\")\n",
    "print(\"Accuracy on training set is : {}\".format(trainingScore/iteration))\n",
    "print(\"Accuracy on test set is : {}\".format(testingScore/iteration))\n",
    "print(classification_report(yTest, yPredict, digits=3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 跨数据集"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-07T14:51:52.930774Z",
     "start_time": "2020-07-07T14:51:38.616598Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results of Logistic Reg\n",
      "Test Results of Poor Road\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0      0.929     0.998     0.962     29138\n",
      "         1.0      0.795     0.324     0.460      2540\n",
      "         2.0      0.500     0.013     0.026       670\n",
      "\n",
      "    accuracy                          0.924     32348\n",
      "   macro avg      0.741     0.445     0.483     32348\n",
      "weighted avg      0.910     0.924     0.903     32348\n",
      "\n",
      "Test Results of Bad Road\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0      0.971     0.982     0.976     86468\n",
      "         1.0      0.879     0.821     0.849     14326\n",
      "         2.0      0.000     0.000     0.000        58\n",
      "\n",
      "    accuracy                          0.958    100852\n",
      "   macro avg      0.617     0.601     0.608    100852\n",
      "weighted avg      0.957     0.958     0.958    100852\n",
      "\n"
     ]
    }
   ],
   "source": [
    "iteration = 100\n",
    "yTestPoor, yPredictPoor = [], []\n",
    "yTestBad, yPredictBad = [], []\n",
    "trainingScore, testingScore = 0, 0\n",
    "for i in range(iteration):\n",
    "    permutation = np.random.permutation(features.shape[0])\n",
    "    X = features[permutation, :]\n",
    "    Y = dataLabel[permutation, :]\n",
    "\n",
    "    X_train, X_test, Y_train, Y_test = train_test_split(\n",
    "        X, Y, test_size=0.3, random_state=0)\n",
    "    clf = LogisticRegression(random_state=0)\n",
    "    clf.fit(X_train, Y_train[:, 0])\n",
    "    # 分割测试集为分别来自bad和poor的测试集\n",
    "    X_test_poor, Y_test_poor, X_test_bad, Y_test_bad = [], [], [], []\n",
    "    for idx in range(X_test.shape[0]):\n",
    "        if Y_test[idx][2] == 1:\n",
    "            X_test_poor.append(X_test[idx, :])\n",
    "            Y_test_poor.append(Y_test[idx, :])\n",
    "        else:\n",
    "            X_test_bad.append(X_test[idx, :])\n",
    "            Y_test_bad.append(Y_test[idx, :])\n",
    "    # poor\n",
    "    X_test_poor = np.array(X_test_poor)\n",
    "    Y_test_poor = np.array(Y_test_poor)\n",
    "    Y_test_poor_pred = clf.predict(X_test_poor)\n",
    "    yTestPoor.extend(Y_test_poor[:, 0])\n",
    "    yPredictPoor.extend(Y_test_poor_pred)\n",
    "    # bad\n",
    "    X_test_bad = np.array(X_test_bad)\n",
    "    Y_test_bad = np.array(Y_test_bad)\n",
    "    Y_test_bad_pred = clf.predict(X_test_bad)\n",
    "    yTestBad.extend(Y_test_bad[:, 0])\n",
    "    yPredictBad.extend(Y_test_bad_pred)\n",
    "\n",
    "print(\"Results of Logistic Reg\")\n",
    "print(\"Test Results of Poor Road\")\n",
    "print(classification_report(yTestPoor, yPredictPoor, digits=3))\n",
    "print(\"Test Results of Bad Road\")\n",
    "print(classification_report(yTestBad, yPredictBad, digits=3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-07T14:52:09.663054Z",
     "start_time": "2020-07-07T14:51:52.932741Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results of Support Vector Machine\n",
      "Test Results of Poor Road\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0      0.912     0.999     0.953     29242\n",
      "         1.0      0.822     0.114     0.200      2443\n",
      "         2.0      0.000     0.000     0.000       704\n",
      "\n",
      "    accuracy                          0.911     32389\n",
      "   macro avg      0.578     0.371     0.384     32389\n",
      "weighted avg      0.885     0.911     0.876     32389\n",
      "\n",
      "Test Results of Bad Road\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0      0.933     0.998     0.964     86448\n",
      "         1.0      0.982     0.568     0.719     14306\n",
      "         2.0      0.000     0.000     0.000        57\n",
      "\n",
      "    accuracy                          0.937    100811\n",
      "   macro avg      0.638     0.522     0.561    100811\n",
      "weighted avg      0.939     0.937     0.929    100811\n",
      "\n"
     ]
    }
   ],
   "source": [
    "iteration = 100\n",
    "yTestPoor, yPredictPoor = [], []\n",
    "yTestBad, yPredictBad = [], []\n",
    "trainingScore, testingScore = 0, 0\n",
    "for i in range(iteration):\n",
    "    permutation = np.random.permutation(features.shape[0])\n",
    "    X = features[permutation, :]\n",
    "    Y = dataLabel[permutation, :]\n",
    "\n",
    "    X_train, X_test, Y_train, Y_test = train_test_split(\n",
    "        X, Y, test_size=0.3, random_state=0)\n",
    "    clf = svm.SVC()\n",
    "    clf.fit(X_train, Y_train[:, 0])\n",
    "    # 分割测试集为分别来自bad和poor的测试集\n",
    "    X_test_poor, Y_test_poor, X_test_bad, Y_test_bad = [], [], [], []\n",
    "    for idx in range(X_test.shape[0]):\n",
    "        if Y_test[idx][2] == 1:\n",
    "            X_test_poor.append(X_test[idx, :])\n",
    "            Y_test_poor.append(Y_test[idx, :])\n",
    "        else:\n",
    "            X_test_bad.append(X_test[idx, :])\n",
    "            Y_test_bad.append(Y_test[idx, :])\n",
    "    # poor\n",
    "    X_test_poor = np.array(X_test_poor)\n",
    "    Y_test_poor = np.array(Y_test_poor)\n",
    "    Y_test_poor_pred = clf.predict(X_test_poor)\n",
    "    yTestPoor.extend(Y_test_poor[:, 0])\n",
    "    yPredictPoor.extend(Y_test_poor_pred)\n",
    "    # bad\n",
    "    X_test_bad = np.array(X_test_bad)\n",
    "    Y_test_bad = np.array(Y_test_bad)\n",
    "    Y_test_bad_pred = clf.predict(X_test_bad)\n",
    "    yTestBad.extend(Y_test_bad[:, 0])\n",
    "    yPredictBad.extend(Y_test_bad_pred)\n",
    "\n",
    "print(\"Results of Support Vector Machine\")\n",
    "print(\"Test Results of Poor Road\")\n",
    "print(classification_report(yTestPoor, yPredictPoor, digits=3))\n",
    "print(\"Test Results of Bad Road\")\n",
    "print(classification_report(yTestBad, yPredictBad, digits=3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-07T14:54:04.278826Z",
     "start_time": "2020-07-07T14:52:09.664050Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results of RandomForest\n",
      "Test Results of Poor Road\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0      0.925     0.998     0.960     28988\n",
      "         1.0      0.806     0.293     0.429      2505\n",
      "         2.0      1.000     0.083     0.153       722\n",
      "\n",
      "    accuracy                          0.922     32215\n",
      "   macro avg      0.911     0.458     0.514     32215\n",
      "weighted avg      0.918     0.922     0.901     32215\n",
      "\n",
      "Test Results of Bad Road\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0      0.971     0.981     0.976     86624\n",
      "         1.0      0.876     0.828     0.851     14305\n",
      "         2.0      0.000     0.000     0.000        56\n",
      "\n",
      "    accuracy                          0.959    100985\n",
      "   macro avg      0.616     0.603     0.609    100985\n",
      "weighted avg      0.957     0.959     0.958    100985\n",
      "\n"
     ]
    }
   ],
   "source": [
    "iteration = 100\n",
    "yTestPoor, yPredictPoor = [], []\n",
    "yTestBad, yPredictBad = [], []\n",
    "trainingScore, testingScore = 0, 0\n",
    "for i in range(iteration):\n",
    "    permutation = np.random.permutation(features.shape[0])\n",
    "    X = features[permutation, :]\n",
    "    Y = dataLabel[permutation, :]\n",
    "\n",
    "    X_train, X_test, Y_train, Y_test = train_test_split(\n",
    "        X, Y, test_size=0.3, random_state=0)\n",
    "    clf =RandomForestClassifier(n_estimators=100)\n",
    "    clf.fit(X_train, Y_train[:, 0])\n",
    "    # 分割测试集为分别来自bad和poor的测试集\n",
    "    X_test_poor, Y_test_poor, X_test_bad, Y_test_bad = [], [], [], []\n",
    "    for idx in range(X_test.shape[0]):\n",
    "        if Y_test[idx][2] == 1:\n",
    "            X_test_poor.append(X_test[idx, :])\n",
    "            Y_test_poor.append(Y_test[idx, :])\n",
    "        else:\n",
    "            X_test_bad.append(X_test[idx, :])\n",
    "            Y_test_bad.append(Y_test[idx, :])\n",
    "    # poor\n",
    "    X_test_poor = np.array(X_test_poor)\n",
    "    Y_test_poor = np.array(Y_test_poor)\n",
    "    Y_test_poor_pred = clf.predict(X_test_poor)\n",
    "    yTestPoor.extend(Y_test_poor[:, 0])\n",
    "    yPredictPoor.extend(Y_test_poor_pred)\n",
    "    # bad\n",
    "    X_test_bad = np.array(X_test_bad)\n",
    "    Y_test_bad = np.array(Y_test_bad)\n",
    "    Y_test_bad_pred = clf.predict(X_test_bad)\n",
    "    yTestBad.extend(Y_test_bad[:, 0])\n",
    "    yPredictBad.extend(Y_test_bad_pred)\n",
    "\n",
    "print(\"Results of RandomForest\")\n",
    "print(\"Test Results of Poor Road\")\n",
    "print(classification_report(yTestPoor, yPredictPoor, digits=3))\n",
    "print(\"Test Results of Bad Road\")\n",
    "print(classification_report(yTestBad, yPredictBad, digits=3))"
   ]
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "307.2px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
