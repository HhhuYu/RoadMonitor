{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-23T08:12:59.970485Z",
     "start_time": "2020-06-23T08:12:59.952494Z"
    }
   },
   "outputs": [],
   "source": [
    "from siml.sk_utils import *\n",
    "from siml.signal_analysis_utils import *\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import warnings\n",
    "import pywt\n",
    "import scipy.stats\n",
    "import pandas as pd\n",
    "from collections import defaultdict, Counter\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.gaussian_process import GaussianProcessClassifier\n",
    "from sklearn.ensemble import IsolationForest\n",
    "from sklearn import svm\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import Normalizer\n",
    "\n",
    "from sklearn.metrics import classification_report\n",
    "from detecta import detect_peaks\n",
    "from scipy.fftpack import fft\n",
    "from scipy.signal import welch\n",
    "from sklearn.model_selection import train_test_split  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 加载数据文件"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-23T08:12:59.995423Z",
     "start_time": "2020-06-23T08:12:59.991428Z"
    }
   },
   "outputs": [],
   "source": [
    "typeDescription = {\n",
    "    0: 'normal',\n",
    "    1: 'pothole',\n",
    "    2: 'transverse',\n",
    "}\n",
    "\n",
    "def readFile(filename):\n",
    "    return np.loadtxt(filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-23T08:13:00.667611Z",
     "start_time": "2020-06-23T08:12:59.998401Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "数据集： (4088, 64, 3)\n",
      "{'normal': 3601, 'pothole': 474, 'transverse': 13}\n"
     ]
    }
   ],
   "source": [
    "folderPath = '../../data/Final_Version/all/datasets/'\n",
    "dataFile = ['dataX.txt', 'dataY.txt', 'dataZ.txt']\n",
    "labelFile = ['dataLabel.txt']\n",
    "\n",
    "signals = []\n",
    "for file in dataFile:\n",
    "    dataPath = folderPath+file\n",
    "    signals.append(np.loadtxt(dataPath))\n",
    "signals = np.transpose(np.array(signals), (1, 2, 0))\n",
    "print('数据集：', signals.shape)\n",
    "\n",
    "labelFilePath = folderPath+labelFile[0]\n",
    "dataLabel = np.loadtxt(labelFilePath)\n",
    "anomalyType = list(dataLabel[:,0])\n",
    "\n",
    "dic = {}\n",
    "temp = Counter(anomalyType)\n",
    "for key in temp.keys():\n",
    "    dic[typeDescription[key]] = temp[key]\n",
    "print(dic)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 特征提取"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 频域特征"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-23T08:13:00.689551Z",
     "start_time": "2020-06-23T08:13:00.670616Z"
    }
   },
   "outputs": [],
   "source": [
    "N = 64 #样本数\n",
    "f_s = 50 #采样频率\n",
    "denominator = 10\n",
    "\n",
    "#时域\n",
    "def get_values(y_values, N, f_s):\n",
    "    f_values = np.linspace(0.0, N/f_s, N)\n",
    "    return f_values, y_values\n",
    "\n",
    "#FFT\n",
    "def get_fft_values(y_values, N, f_s):\n",
    "    f_values = np.linspace(0.0, f_s/2.0, N//2)\n",
    "    fft_values_ = fft(y_values)\n",
    "    fft_values = 2.0/N * np.abs(fft_values_[0:N//2])\n",
    "    return f_values, fft_values\n",
    "\n",
    "#PSD\n",
    "def get_psd_values(y_values, N, f_s):\n",
    "    f_values, psd_values = welch(y_values, fs=f_s)\n",
    "    return f_values, psd_values\n",
    "\n",
    "#Autocorrelation\n",
    "def autocorr(x):\n",
    "    result = np.correlate(x, x, mode='full')\n",
    "    return result[len(result)//2:]\n",
    " \n",
    "def get_autocorr_values(y_values, N, f_s):\n",
    "    autocorr_values = autocorr(y_values)\n",
    "    x_values = np.array([ 1.0*jj/f_s for jj in range(0, N)])\n",
    "    return x_values, autocorr_values\n",
    "\n",
    "\n",
    "def get_first_n_peaks(x, y, no_peaks=5):\n",
    "    x_, y_ = list(x), list(y)\n",
    "    if len(x_) >= no_peaks:\n",
    "        return x_[:no_peaks], y_[:no_peaks]\n",
    "    else:#少于5个peaks，以0填充\n",
    "        missing_no_peaks = no_peaks-len(x_)\n",
    "        return x_ + [0]*missing_no_peaks, y_ + [0]*missing_no_peaks\n",
    "\n",
    "\n",
    "def get_features1(x_values, y_values, mph):\n",
    "    indices_peaks = detect_peaks(y_values, mph=mph)\n",
    "    peaks_x, peaks_y = get_first_n_peaks(\n",
    "        x_values[indices_peaks], y_values[indices_peaks])\n",
    "    return peaks_x + peaks_y\n",
    "\n",
    "\n",
    "def extract_features1(dataset, N, f_s, denominator):\n",
    "    percentile = 5\n",
    "    list_of_features = []\n",
    "\n",
    "    for signal_no in range(0, len(dataset)):\n",
    "        features = [] #5*2*3*3\n",
    "        \n",
    "        for signal_comp in range(0, dataset.shape[2]):\n",
    "            signal = dataset[signal_no, :, signal_comp]\n",
    "\n",
    "            signal_min = np.nanpercentile(signal, percentile)\n",
    "            signal_max = np.nanpercentile(signal, 100-percentile)\n",
    "            #ijk = (100 - 2*percentile)/10\n",
    "            #set minimum peak height\n",
    "            mph = signal_min + (signal_max - signal_min)/denominator\n",
    "\n",
    "            features += get_features1(*get_psd_values(signal, N, f_s), mph)\n",
    "            features += get_features1(*get_fft_values(signal, N, f_s), mph)\n",
    "            features += get_features1(*get_autocorr_values(signal, N, f_s), mph)\n",
    "        list_of_features.append(features)\n",
    "    return np.array(list_of_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-23T08:13:09.512004Z",
     "start_time": "2020-06-23T08:13:00.691546Z"
    }
   },
   "outputs": [],
   "source": [
    "features1 = extract_features1(\n",
    "    signals, N, f_s, denominator)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 小波域特征"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-23T08:13:09.526977Z",
     "start_time": "2020-06-23T08:13:09.513001Z"
    }
   },
   "outputs": [],
   "source": [
    "def calculate_entropy(list_values):\n",
    "    counter_values = Counter(list_values).most_common()\n",
    "    # print(counter_values)\n",
    "    probabilities = [elem[1]/len(list_values) for elem in counter_values]\n",
    "    # print(probabilities)\n",
    "    entropy = scipy.stats.entropy(probabilities)\n",
    "    return entropy\n",
    "\n",
    "\n",
    "def calculate_statistics(list_values):\n",
    "    n5 = np.nanpercentile(list_values, 5)\n",
    "    n25 = np.nanpercentile(list_values, 25)\n",
    "    n75 = np.nanpercentile(list_values, 75)\n",
    "    n95 = np.nanpercentile(list_values, 95)\n",
    "    median = np.nanpercentile(list_values, 50)\n",
    "    mean = np.nanmean(list_values)\n",
    "    std = np.nanstd(list_values)\n",
    "    var = np.nanvar(list_values)\n",
    "    rms = np.nanmean(np.sqrt(list_values**2))\n",
    "    return [n5, n25, n75, n95, median, mean, std, var, rms]\n",
    "\n",
    "\n",
    "def calculate_crossings(list_values):\n",
    "    zero_crossing_indices = np.nonzero(np.diff(np.array(list_values) > 0))[0]\n",
    "    no_zero_crossings = len(zero_crossing_indices)\n",
    "    mean_crossing_indices = np.nonzero(\n",
    "        np.diff(np.array(list_values) > np.nanmean(list_values)))[0]\n",
    "    no_mean_crossings = len(mean_crossing_indices)\n",
    "    return [no_zero_crossings, no_mean_crossings]\n",
    "\n",
    "\n",
    "def get_features2(list_values):\n",
    "    entropy = calculate_entropy(list_values)\n",
    "    crossings = calculate_crossings(list_values)\n",
    "    statistics = calculate_statistics(list_values)\n",
    "    return [entropy] + crossings + statistics\n",
    "\n",
    "\n",
    "def extract_features2(dataset, waveletname):\n",
    "    uci_har_features = []\n",
    "    for signal_no in range(0, len(dataset)):\n",
    "        features = []\n",
    "        for signal_comp in range(0, dataset.shape[2]):\n",
    "            signal = dataset[signal_no, :, signal_comp]\n",
    "            # 小波变换，返回长度为5，元素为单列array的list\n",
    "            list_coeff = pywt.wavedec(signal, waveletname)\n",
    "            #print(len(list_coeff))\n",
    "            for coeff in list_coeff:\n",
    "                # 对小波变换后的系数取其特征：entropy+statistics+crossings\n",
    "                features += get_features2(coeff)\n",
    "                #print(len(features))\n",
    "        uci_har_features.append(features)\n",
    "    X = np.array(uci_har_features)\n",
    "    return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-23T08:13:59.473295Z",
     "start_time": "2020-06-23T08:13:09.528958Z"
    }
   },
   "outputs": [],
   "source": [
    "waveletname = 'rbio3.1'\n",
    "features2 = extract_features2(signals,waveletname)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-23T08:13:59.484257Z",
     "start_time": "2020-06-23T08:13:59.475280Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4088, 90)\n",
      "(4088, 180)\n",
      "(4088, 270)\n"
     ]
    }
   ],
   "source": [
    "print(features1.shape)\n",
    "print(features2.shape)\n",
    "features = np.hstack((features1,features2))\n",
    "print(features.shape)\n",
    "labels = np.array(anomalyType)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-23T08:13:59.539110Z",
     "start_time": "2020-06-23T08:13:59.487249Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.02863046, 0.04771743, 0.0763479 , ..., 0.01433826, 0.01682968,\n",
       "        0.01083583],\n",
       "       [0.03640027, 0.10010075, 0.1456011 , ..., 0.01505635, 0.01946188,\n",
       "        0.01116774],\n",
       "       [0.03780414, 0.06615724, 0.09451035, ..., 0.01879833, 0.02921117,\n",
       "        0.0126046 ],\n",
       "       ...,\n",
       "       [0.02100236, 0.04200472, 0.06300707, ..., 0.01634429, 0.02981091,\n",
       "        0.012177  ],\n",
       "       [0.01916063, 0.08622285, 0.12454412, ..., 0.01677544, 0.0229487 ,\n",
       "        0.01196653],\n",
       "       [0.02907852, 0.08723557, 0.14539261, ..., 0.01156783, 0.01078556,\n",
       "        0.00923979]])"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "StandardScaler().fit_transform(features)\n",
    "Normalizer().fit_transform(features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 信号分类"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-23T08:13:59.544097Z",
     "start_time": "2020-06-23T08:13:59.540106Z"
    }
   },
   "outputs": [],
   "source": [
    "def randomize(features, labels):\n",
    "    permutation = np.random.permutation(labels.shape[0])\n",
    "    shuffled_features = features[permutation, :]\n",
    "    shuffled_labels = labels[permutation]\n",
    "    return shuffled_features, shuffled_labels\n",
    "\n",
    "iteration = 100\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-23T08:14:23.638502Z",
     "start_time": "2020-06-23T08:13:59.546102Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results of Logistic Regression\n",
      "Accuracy on training set is : 0.9668647326109747\n",
      "Accuracy on test set is : 0.9471801140994297\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0      0.964     0.979     0.971    107974\n",
      "         1.0      0.817     0.733     0.773     14330\n",
      "         2.0      0.006     0.003     0.004       396\n",
      "\n",
      "    accuracy                          0.947    122700\n",
      "   macro avg      0.596     0.572     0.583    122700\n",
      "weighted avg      0.944     0.947     0.945    122700\n",
      "\n"
     ]
    }
   ],
   "source": [
    "yTest, yPredict = [], []\n",
    "trainingScore, testingScore = 0, 0\n",
    "for i in range(iteration):\n",
    "    X, Y = randomize(features,labels)\n",
    "    X_train,X_test,Y_train,Y_test = train_test_split(X,Y,test_size=0.3,random_state=0)\n",
    "    clf = LogisticRegression(random_state=0)\n",
    "    clf.fit(X_train, Y_train)\n",
    "    Y_test_pred = clf.predict(X_test)\n",
    "    yTest.extend(Y_test)\n",
    "    yPredict.extend(Y_test_pred)\n",
    "    trainingScore += clf.score(X_train, Y_train)\n",
    "    testingScore += clf.score(X_test, Y_test)\n",
    "print(\"Results of Logistic Regression\")\n",
    "print(\"Accuracy on training set is : {}\".format(trainingScore/iteration))\n",
    "print(\"Accuracy on test set is : {}\".format(testingScore/iteration))\n",
    "print(classification_report(yTest, yPredict,digits=3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Support Vector Machine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-23T08:16:18.857410Z",
     "start_time": "2020-06-23T08:14:23.641494Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results of Support Vector Machine\n",
      "Accuracy on training set is : 0.954267738552953\n",
      "Accuracy on test set is : 0.9512469437652815\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0      0.955     0.993     0.973    108039\n",
      "         1.0      0.914     0.663     0.769     14291\n",
      "         2.0      0.000     0.000     0.000       370\n",
      "\n",
      "    accuracy                          0.951    122700\n",
      "   macro avg      0.623     0.552     0.581    122700\n",
      "weighted avg      0.947     0.951     0.947    122700\n",
      "\n"
     ]
    }
   ],
   "source": [
    "yTest, yPredict = [], []\n",
    "trainingScore, testingScore = 0, 0\n",
    "for i in range(iteration):\n",
    "    X, Y = randomize(features,labels)\n",
    "    X_train,X_test,Y_train,Y_test = train_test_split(X,Y,test_size=0.3,random_state=0)\n",
    "    clf = svm.SVC()\n",
    "    clf.fit(X_train, Y_train)\n",
    "    Y_test_pred = clf.predict(X_test)\n",
    "    yTest.extend(Y_test)\n",
    "    yPredict.extend(Y_test_pred)\n",
    "    trainingScore += clf.score(X_train, Y_train)\n",
    "    testingScore += clf.score(X_test, Y_test)\n",
    "print(\"Results of Support Vector Machine\")  \n",
    "print(\"Accuracy on training set is : {}\".format(trainingScore/iteration))\n",
    "print(\"Accuracy on test set is : {}\".format(testingScore/iteration))\n",
    "print(classification_report(yTest, yPredict,digits=3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-23T08:21:34.491407Z",
     "start_time": "2020-06-23T08:16:18.858407Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results of RandomForest\n",
      "Accuracy on training set is : 0.9999860188745191\n",
      "Accuracy on test set is : 0.9565851670741652\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0      0.964     0.989     0.976    108110\n",
      "         1.0      0.889     0.738     0.807     14200\n",
      "         2.0      0.000     0.000     0.000       390\n",
      "\n",
      "    accuracy                          0.957    122700\n",
      "   macro avg      0.618     0.576     0.594    122700\n",
      "weighted avg      0.952     0.957     0.953    122700\n",
      "\n"
     ]
    }
   ],
   "source": [
    "yTest, yPredict = [], []\n",
    "trainingScore, testingScore = 0, 0\n",
    "for i in range(iteration):\n",
    "    X, Y = randomize(features,labels)\n",
    "    X_train,X_test,Y_train,Y_test = train_test_split(X,Y,test_size=0.3,random_state=0)\n",
    "    clf = RandomForestClassifier(n_estimators=100)\n",
    "    clf.fit(X_train, Y_train)\n",
    "    Y_test_pred = clf.predict(X_test)\n",
    "    yTest.extend(Y_test)\n",
    "    yPredict.extend(Y_test_pred)\n",
    "    trainingScore += clf.score(X_train, Y_train)\n",
    "    testingScore += clf.score(X_test, Y_test)\n",
    "print(\"Results of RandomForest\")\n",
    "print(\"Accuracy on training set is : {}\".format(trainingScore/iteration))\n",
    "print(\"Accuracy on test set is : {}\".format(testingScore/iteration))\n",
    "print(classification_report(yTest, yPredict,digits=3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comparing all models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-23T08:22:27.280402Z",
     "start_time": "2020-06-23T08:21:34.492372Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trained Gradient Boosting Classifier in 43.73 s\n",
      "trained Random Forest in 2.91 s\n",
      "trained Logistic Regression in 0.30 s\n",
      "trained Nearest Neighbors in 0.16 s\n",
      "trained Decision Tree in 1.27 s\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>classifier</th>\n",
       "      <th>train_score</th>\n",
       "      <th>test_score</th>\n",
       "      <th>train_time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Random Forest</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.955990</td>\n",
       "      <td>2.905287</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Gradient Boosting Classifier</td>\n",
       "      <td>0.998602</td>\n",
       "      <td>0.942950</td>\n",
       "      <td>43.731056</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Logistic Regression</td>\n",
       "      <td>0.968193</td>\n",
       "      <td>0.941320</td>\n",
       "      <td>0.303719</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Nearest Neighbors</td>\n",
       "      <td>0.951416</td>\n",
       "      <td>0.938060</td>\n",
       "      <td>0.160129</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Decision Tree</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.914425</td>\n",
       "      <td>1.270188</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                     classifier  train_score  test_score  train_time\n",
       "1                 Random Forest     1.000000    0.955990    2.905287\n",
       "0  Gradient Boosting Classifier     0.998602    0.942950   43.731056\n",
       "2           Logistic Regression     0.968193    0.941320    0.303719\n",
       "3             Nearest Neighbors     0.951416    0.938060    0.160129\n",
       "4                 Decision Tree     1.000000    0.914425    1.270188"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "dict_results = batch_classify(X_train, Y_train, X_test, Y_test)\n",
    "display_dict_models(dict_results)"
   ]
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "307.2px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
